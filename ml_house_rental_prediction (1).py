# -*- coding: utf-8 -*-
"""ML-house-rental-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ia566iKHcrSSObNdvb4H4OQO9_x2WV-J

#  Proyek Pertama : Predictive Analytics (House rental prediction ML )

Nama : Yuliana Habibah

Email: yuliana0669ft.2022@student.uny.ac.id

Dataset ini diambil dari : https://www.kaggle.com/datasets/iamsouravbanerjee/house-rent-prediction-dataset

##**Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# %matplotlib inline

"""##**Data Load**"""

!gdown --id "11vCKuk81QvTQcwibMX7Wo6k4q0HDzLOz"

df = pd.read_csv("/content/House_Rent_Dataset.csv")
df.head()

"""##**Data Understanding & Removing Outlier**"""

df.shape

df.info()

# Hapus kolom yang tidak berpengaruh terhadap harga sewa: 'Posted On' dan 'Point of Contact'
df.drop(columns=['Posted On', 'Point of Contact'], inplace=True)

"""##**Unvariate Analysis**"""

df.groupby('Area Type')['Area Type'].agg('count')

# Hapus baris yang memiliki nilai 'Built Area' pada kolom 'Area Type' karena jumlahnya hanya 2
df = df[df['Area Type'] != 'Built Area']

df.groupby('Area Type')['Area Type'].agg('count')

df.groupby('City')['City'].agg('count')

df.groupby('Furnishing Status')['Furnishing Status'].agg('count')

df.groupby('Tenant Preferred')['Tenant Preferred'].agg('count')

df.groupby('Floor')['Floor'].agg('count')

df.groupby('Area Locality')['Area Locality'].agg('count')

# Drop kolom dengan terlalu banyak nilai unik: 'Floor' dan 'Area Locality'
df.drop(columns=['Floor', 'Area Locality'], inplace=True)

df.head()

df.hist(bins=50, figsize=(10,10))  # Membuat histogram untuk semua kolom numerik dengan 50 batang dan ukuran 10x10
plt.ticklabel_format(useOffset=False, style='plain')  # Menonaktifkan offset dan menampilkan angka dalam format standar
plt.show()  # Menampilkan plot

df.Rent.describe().apply(lambda x: format(x, 'f'))

"""##**Multivariate Analysis**"""

# Menambahkan fitur baru price per sqft
df['Price_per_sqft'] = df['Rent']*1000/df['Size']

df.head()

# Tampilkan 5 data pertama dengan rasio luas per BHK kurang dari 300 sqft (dianggap tidak wajar)
df[df['Size'] / df['BHK'] < 300].head()

df.shape

# Menghapus size per BHK outlier
df1 = df[~(df.Size/df.BHK < 300)]
df1.head()

df1.shape

# Mendeteksi price per sqft outlier
df1.Price_per_sqft.describe().apply(lambda x: format(x, 'f'))

def remove_pps_outliers(df):
    df_out = pd.DataFrame()  # Buat DataFrame kosong untuk menampung hasil akhir
    for key, subdf in df.groupby('City'):  # Kelompokkan berdasarkan kota
        m = np.mean(subdf.Price_per_sqft)  # Rata-rata price/sqft di kota tersebut
        st = np.std(subdf.Price_per_sqft)  # Standar deviasi price/sqft di kota tersebut
        # Filter data yang berada dalam 1 standar deviasi dari rata-rata
        reduced_df = subdf[(subdf.Price_per_sqft > (m - st)) & (subdf.Price_per_sqft <= (m + st))]
        df_out = pd.concat([df_out, reduced_df], ignore_index=True)  # Gabungkan hasilnya ke df_out
    return df_out

# Terapkan fungsi ke df1 dan tampilkan ukuran hasilnya
df2 = remove_pps_outliers(df1)
df2.shape  # Tampilkan jumlah baris dan kolom setelah outlier dihapus

# Mendeteksi bathroom outlier
# 2 BHK dengan 4 kamar mandi itu tidak biasa jadi anggap saja batasnya kamar mandi tidak boleh melebihi jumlah BHK + 2

df2[df2.Bathroom > df2.BHK + 2]

# Hapus data yang memiliki jumlah kamar mandi lebih dari 2 kamar di atas jumlah BHK (misalnya 2 BHK dengan 5+ kamar mandi)
df2 = df2[df2.Bathroom <= df2.BHK + 2]

df2.shape

# Menghilangkan fitur price per sqft karena sudah tidak terpakai
df3 = df2.drop(['Price_per_sqft'], axis = 'columns')

# Melihat kolerasi antara fitur numerik dengan fitur target (harga)
plt.figure(figsize=(10, 8))

# Select only the numeric columns before calculating correlation
correlation_matrix = df3.select_dtypes(include=np.number).corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)
plt.show()

# Ambil daftar kolom yang bertipe kategorik (object)
cat_features = df2.select_dtypes(include='object').columns.to_list()

# Loop untuk setiap fitur kategorik
for col in cat_features:
    # Buat grafik batang rata-rata Rent per kategori fitur tersebut
    sns.catplot(
        x=col, y="Rent", kind="bar", dodge=False,
        height=4, aspect=3,
        data=df2, palette="Set3"
    )
    # Beri judul grafik sesuai dengan fitur yang sedang ditampilkan
    plt.title(f"Rata-rata 'Rent' Relatif terhadap - {col}")

"""##**Data Preparation**

##One hot enconding
"""

df3 = pd.get_dummies(data =  df3, columns = ['Area Type'])
df3 = pd.get_dummies(data =  df3, columns = ['City'])
df3 = pd.get_dummies(data =  df3, columns = ['Furnishing Status'])
df3 = pd.get_dummies(data =  df3, columns = ['Tenant Preferred'])

df3.head()

"""##Train Test Split"""

# Import train_test_split
from sklearn.model_selection import train_test_split
# Install scikit-learn if it's not already installed
!pip install scikit-learn

X = df3.drop(["Rent"], axis=1)       # Menghapus kolom 'Rent' dari df3 untuk dijadikan fitur (X)
y = df3["Rent"]                      # Menyimpan kolom 'Rent' sebagai target (y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.05, random_state=123
)
# Membagi data menjadi 95% data latih dan 5% data uji
# random_state digunakan agar pembagian data selalu konsisten

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""##Normalization"""

from sklearn.preprocessing import StandardScaler  # Impor pustaka untuk standardisasi

numerical_features = ['BHK', 'Size', 'Bathroom']  # Daftar fitur numerik yang akan dinormalisasi

scaler = StandardScaler()                         # Membuat objek StandardScaler
scaler.fit(X_train[numerical_features])           # Menghitung rata-rata dan standar deviasi dari data pelatihan

# Menerapkan transformasi ke data pelatihan
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])

# Menampilkan hasil normalisasi lima baris pertama
X_train[numerical_features].head()

# Normalisasi data test
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""##**Modeling**

##Grid Search
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor

from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor


from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit

def grid_search_model(X,y):
    algos = {
        'knn': {
            'model': KNeighborsRegressor(),
            'params': {
                'n_neighbors': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
            }
        },
        'boosting': {
            'model': AdaBoostRegressor(),
            'params': {
                'learning_rate' : [0.1, 0.05, 0.01, 0.05, 0.001],
                'n_estimators': [25, 50, 75, 100],
                'random_state': [11, 33, 55, 77]
            }
        },
        'random_forest': {
            'model': RandomForestRegressor(),
            'params': {
                'n_estimators': [25, 50, 75, 100],
                'max_depth' : [8, 16, 32, 64],
                'random_state': [11, 33, 55, 77],
            }
        }

    }

    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.05, random_state=123)
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

grid_search_model(X,y)

"""##Optimized Model"""

acc = pd.DataFrame(index=['accuracy'])

from sklearn.metrics import mean_squared_error

# Membuat model KNN dengan jumlah tetangga sebanyak 7
model_knn = KNeighborsRegressor(n_neighbors=7)

# Melatih model menggunakan data pelatihan
model_knn.fit(X_train, y_train)

# Menyimpan skor akurasi model KNN pada data uji ke dalam DataFrame acc
acc.loc['accuracy', 'knn'] = model_knn.score(X_test, y_test)

# Menampilkan skor akurasi model KNN pada data uji
model_knn.score(X_test, y_test)

rf = RandomForestRegressor(n_estimators = 50, max_depth = 8, random_state = 11)
rf.fit(X_train, y_train)
acc.loc['accuracy', 'rf'] = rf.score(X_test,y_test)
rf.score(X_test,y_test)

# Membuat model AdaBoost Regressor dengan 25 estimator, learning rate 0.001, dan random_state 11
boosting = AdaBoostRegressor(n_estimators=25, learning_rate=0.001, random_state=11)

# Melatih model boosting dengan data pelatihan
boosting.fit(X_train, y_train)

# Menyimpan nilai akurasi model pada data uji ke dalam DataFrame 'acc'
acc.loc['accuracy', 'boosting'] = boosting.score(X_test, y_test)

# Menampilkan skor akurasi model pada data uji
boosting.score(X_test, y_test)

"""##**Evaluasi**"""

# Akurasi dari model
acc

# Membuat DataFrame untuk menyimpan nilai Mean Squared Error (MSE) pada data train dan test untuk tiap model
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN', 'RF', 'Boosting'])

# Membuat dictionary model untuk memudahkan iterasi
# Mengganti 'knn' dengan 'model_knn' yang merupakan nama variabel yang benar
model_dict = {'KNN': model_knn, 'RF': rf, 'Boosting': boosting}

# Menghitung MSE pada data training dan testing untuk setiap model, lalu menyimpannya di DataFrame mse
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train)) / 1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test)) / 1e3

# Menampilkan tabel MSE
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

# Mengambil sampel data X_test dari indeks 5 sampai 9 untuk prediksi
prediksi = X_test.iloc[5:10].copy()

# Membuat dictionary untuk menyimpan nilai aktual dari y_test
pred_dict = {'y_true': y_test[5:10]}

# Melakukan prediksi pada data sampel dengan setiap model dan menyimpan hasilnya dalam dictionary
for name, model in model_dict.items():
    pred_dict['prediksi_' + name] = model.predict(prediksi).round(1)

# Mengubah dictionary hasil prediksi dan nilai sebenarnya menjadi DataFrame untuk memudahkan perbandingan
pd.DataFrame(pred_dict)